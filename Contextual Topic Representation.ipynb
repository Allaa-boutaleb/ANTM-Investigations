{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c928477",
   "metadata": {},
   "source": [
    "# Topic representation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f654e8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T12:58:26.209197Z",
     "start_time": "2023-06-22T12:58:21.233101Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dependencies import *\n",
    "import warnings\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "from hdbscan import HDBSCAN\n",
    "from hdbscan.flat import HDBSCAN_flat\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067783fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T12:58:32.161079Z",
     "start_time": "2023-06-22T12:58:32.144357Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f299326",
   "metadata": {},
   "source": [
    "## Fixiating various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6d5f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T12:58:32.470336Z",
     "start_time": "2023-06-22T12:58:32.464962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Umap default parameters\n",
    "umap_n_neighbors = 15\n",
    "umap_dim_size = 5\n",
    "\n",
    "# HDBSCAN Parameters\n",
    "partioned_clusttering_size = 20\n",
    "epsilon=0.0\n",
    "\n",
    "# number of words to represent topic\n",
    "num_words = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3266ad",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6484e8",
   "metadata": {
    "code_folding": [],
    "ExecuteTime": {
     "end_time": "2023-06-22T12:58:33.901322Z",
     "start_time": "2023-06-22T12:58:33.898558Z"
    }
   },
   "outputs": [],
   "source": [
    "## Redefining a few functions from dependencies.py\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def preprocessing_documents(document_list):\n",
    "    cleaned_documents=[doc.lower() for doc in document_list]\n",
    "    cleaned_documents = [doc.replace(\"\\n\", \" \") for doc in cleaned_documents]\n",
    "    cleaned_documents = [doc.replace(\"\\t\", \" \") for doc in cleaned_documents]\n",
    "    cleaned_documents = [re.sub(r'[^A-Za-z0-9 ]+', '', doc) for doc in cleaned_documents]\n",
    "    return cleaned_documents\n",
    "\n",
    "\n",
    "def doucments_lemmatizer(documents_tokens):\n",
    "    tokens=[[WordNetLemmatizer().lemmatize(token) for token in doc_tokens] for doc_tokens in documents_tokens]\n",
    "    return tokens\n",
    "\n",
    "def document_tokenize(cleaned_documents):\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    document_tokens = [word_tokenize(document) for document in cleaned_documents ]\n",
    "    tokens=[[token for token in document_tokens if not token in stop_words] for document_tokens in document_tokens ]\n",
    "    return tokens\n",
    "\n",
    "def calculate_cluster_embeddings(df):\n",
    "    # Assuming the 'C' column contains the cluster ids and 'embedding' contains the embeddings\n",
    "    df['embedding'] = df['embedding'].apply(np.array)  # Ensure the embeddings are numpy arrays\n",
    "    df_grouped = df.groupby('C')['embedding'].apply(np.stack).apply(np.mean, axis=0)\n",
    "    return df_grouped.reset_index()\n",
    "\n",
    "\n",
    "def text_processing(all_documents) :\n",
    "    preprocessed_documents=preprocessing_documents(all_documents)\n",
    "    documents_tokens=document_tokenize(preprocessed_documents)\n",
    "    tokens=doucments_lemmatizer(documents_tokens)\n",
    "    tokens=token_frequency_filter(tokens,10)\n",
    "    dictionary = Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokens]\n",
    "    return tokens,dictionary,corpus\n",
    "\n",
    "\n",
    "# Less aggressive preprocessing, allows less frequent tokens to stay around\n",
    "def text_processing_2(all_documents) :\n",
    "    preprocessed_documents=preprocessing_documents(all_documents)\n",
    "    documents_tokens=document_tokenize(preprocessed_documents)\n",
    "    tokens=doucments_lemmatizer(documents_tokens)\n",
    "    tokens=token_frequency_filter(tokens,5)\n",
    "    dictionary = Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokens]\n",
    "    return tokens,dictionary,corpus\n",
    "\n",
    "# returns whole word embeddings after merging the subtokens\n",
    "def dict_extractor(tokens, embeddings):\n",
    "    ## convert token embedding tensors into numpy arrays\n",
    "    token_embeddings = embeddings.numpy()\n",
    "    my_list = list(zip(tokens, token_embeddings))\n",
    "        \n",
    "    new_list = []\n",
    "    new_list2 = []\n",
    "    i = 0\n",
    "    while i < len(my_list):\n",
    "        if i < len(my_list) - 1 and my_list[i+1][0].startswith('##'):\n",
    "            combined_word = my_list[i][0] + my_list[i+1][0][2:]\n",
    "            float_sum = np.sum([my_list[i][1], my_list[i+1][1]],axis=0)\n",
    "            j = 2\n",
    "            while i+j < len(my_list) and my_list[i+j][0].startswith('##'):\n",
    "                combined_word += my_list[i+j][0][2:]\n",
    "                float_sum = np.sum([float_sum, my_list[i+j][1]],axis=0)\n",
    "                j += 1\n",
    "            i += j\n",
    "            new_list.append(combined_word)\n",
    "            new_list2.append(float_sum / j)\n",
    "        else:\n",
    "            new_list.append(my_list[i][0])\n",
    "            new_list2.append(my_list[i][1])\n",
    "            i += 1\n",
    "    my_list = list(zip(new_list,new_list2))\n",
    "    res = {}\n",
    "    for s, v in my_list:\n",
    "        if s in res: res[s].append(v)\n",
    "        else: res[s] = [v]\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "923c83a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T12:58:34.589689Z",
     "start_time": "2023-06-22T12:58:34.583356Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Return dataframe of topics and their associated word representations\n",
    "\n",
    "def find_nearest_words(topic_df, word_df, documents_per_topic_per_time, k, diversity = 0.0,verbose = False) :\n",
    "    result = []\n",
    "    doc_embeddings = [topic_df.iloc[i].embedding for i in range(len(topic_df))]\n",
    "    words_all = word_df['content'].tolist()\n",
    "    for i,doc_embedding in enumerate(doc_embeddings) :\n",
    "        words_topic = text_processing_2([documents_per_topic_per_time.iloc[i].content])[0][0]\n",
    "        # reduce the candidate space by considering only the words that constitute the docs of the topic\n",
    "        # if number of candidates is lower than k, consider entire dictionary as candidate space\n",
    "        set1 = set(words_topic)\n",
    "        set2 = set(words_all)\n",
    "        intersection = set1 & set2\n",
    "        words = []\n",
    "        for w in words_all :\n",
    "            if w in words_topic :\n",
    "                words.append(w)\n",
    "        if len(words) < k :\n",
    "            words = words_all\n",
    "        df = word_df[word_df['content'].isin(words)]\n",
    "\n",
    "        word_doc_similarity = cosine_similarity(np.array(df['embedding'].tolist()),[doc_embedding])\n",
    "        word_similarity = cosine_similarity(np.array(df['embedding'].tolist()))\n",
    "\n",
    "        # Initialize candidates and already choose best keyword/keyphrase\n",
    "        keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "        candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "        for _ in range(k - 1):\n",
    "            # Extract similarities within candidates and between candidates and selected keywords/phrases\n",
    "            candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "            target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "            # Calculate MMR : diversity is set to 0 for default settings\n",
    "            # increasing it will make the selected words for a topic more diversified\n",
    "            mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "            mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "            # Update keywords & candidates\n",
    "            keywords_idx.append(mmr_idx)\n",
    "            candidates_idx.remove(mmr_idx)\n",
    "        tmp = [words[idx] for idx in keywords_idx]\n",
    "\n",
    "        if verbose :\n",
    "            print(tmp)\n",
    "\n",
    "        result.append((doc_embedding,tmp))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(result, columns=['embedding', 'topic_representation'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "814e9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampled data : 25k documents per year, 2012 to 2017 included.\n",
    "df_sampled = pd.read_pickle('nyt_bert_25k.pkl')\n",
    "df_embedded = df_sampled\n",
    "df = df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "7e2fb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computational reasons, select one year at a time\n",
    "df_embedded = df_embedded[df_embedded['time'] == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "273c8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, dictionary, corpus = text_processing(df_embedded.content.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788907f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do this once, next time, load umap embedding from cell below\n",
    "\n",
    "fit = umap.UMAP(\n",
    "            metric=\"cosine\",\n",
    "            n_neighbors=15,\n",
    "            n_components=5,\n",
    "            n_epochs=200,\n",
    "            random_state=42)\n",
    "\n",
    "umap_embeddings_clustering = fit.fit_transform(df_embedded['embedding'].tolist())\n",
    "pickle.dump(umap_embeddings_clustering, open('2017.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "bcf429f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load umap embedding if it has been already generated\n",
    "umap_embeddings_clustering = pickle.load(open('2017.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46b27a",
   "metadata": {},
   "source": [
    "### HDBSCAN clustering initiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "419b3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "d125c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN\n",
    "partioned_clusttering_size = 20\n",
    "c = hdbscan.HDBSCAN(min_cluster_size=partioned_clusttering_size, metric = \"euclidean\",cluster_selection_method = \"eom\")\n",
    "c.fit(umap_embeddings_clustering)\n",
    "labels = c.labels_\n",
    "probabilities = c.probabilities_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032321e",
   "metadata": {},
   "source": [
    "### Link documents and the clusters they belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "8e1dd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df_embedded.assign(C = labels).sort_values(by=['C'])\n",
    "df_cluster = df_cluster[df_cluster['C'] > -1]\n",
    "df_cluster = df_cluster.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "884cc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df_embedded.content.values\n",
    "topic_ids = np.unique(df_cluster['C'])\n",
    "df_cluster['slice_num'] = 1\n",
    "cluster_df = [df_cluster]\n",
    "documents_per_topic_per_time = rep_prep(cluster_df)\n",
    "output = ctfidf_rp2(dictionary, documents_per_topic_per_time, num_doc=len(df_embedded), num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8217d",
   "metadata": {},
   "source": [
    "### Calculate topic vector for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "0175a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = calculate_cluster_embeddings(df_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543decc",
   "metadata": {},
   "source": [
    "# Method 1 - Calculate word vector for each word (Consider a word as one document) - (non-contextual NEAREST WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "f01c8e70",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the n-closest words that represent each topic\n",
    "# We first need to calculate the embedding of each word\n",
    "\n",
    "df_words = pd.DataFrame(list(dictionary.token2id.keys()), columns=['content'])\n",
    "word_vectors = contextual_embedding(df_words,mode='mpnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "fc261a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_top2vec = find_nearest_words(topic_vectors,word_vectors,documents_per_topic_per_time,k=10,diversity=0.0,verbose=False)\n",
    "res_top2vec = res_top2vec.drop('embedding', axis=1)\n",
    "res_top2vec = res_top2vec.rename(columns={'topic_representation' : 'nearest_words'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3ae4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T20:56:07.059839Z",
     "start_time": "2023-04-15T20:56:07.049153Z"
    }
   },
   "source": [
    "# 2 - Calculate word vector for each word (Consider a word as a max pooling of its various mean pooled embeddings) - (contextual NEAREST WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "a1e6940c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer #, AutoModel, pipeline, BertTokenizer, BertModel\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from collections import Counter\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    # Pass the input through the model\n",
    "    output = model.encode(sentence,output_value=None)\n",
    "    return dict_extractor(tokenizer.convert_ids_to_tokens(output['input_ids'])[1:-1], output['token_embeddings'][1:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e875fed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T18:15:25.060136Z",
     "start_time": "2023-06-22T17:58:32.999094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/25000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87fd6b799d35450a812b723a9220bd57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WARNING : 'we' is a big dictionary! Saving it after helps to avoid having to do this everytime.\n",
    "\n",
    "documents = df_embedded.content.values\n",
    "we = {}\n",
    "for i in tqdm_notebook(range(len(documents))) :\n",
    "    word_embeddings = get_embedding(documents[i])\n",
    "    for word in word_embeddings.keys() :\n",
    "        if word not in we : \n",
    "            we[word] = [word_embeddings[word]]\n",
    "        else :\n",
    "            we[word].append(word_embeddings[word])\n",
    "\n",
    "with open('we_2017.pickle', 'wb') as handle:\n",
    "    pickle.dump(we, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "bccd2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads we if already calculated\n",
    "with open('we_2017.pickle', 'rb') as handle:\n",
    "    we = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "dfc5ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial filtering to keep only words in the dictionary used by the other two methods for a more accurate comparison\n",
    "we_filtered = {}\n",
    "dicto = list(dictionary.token2id.keys())\n",
    "for key, value in we.items():\n",
    "    if key in dicto : we_filtered[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "5926520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helps remove some problematic words that sometimes appear everywhere\n",
    "if 'succinct' in we_filtered : del we_filtered['succinct']\n",
    "if 'darndest' in we_filtered : del we_filtered['darndest']\n",
    "if 'healthlink' in we_filtered : del we_filtered['healthlink']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "113922b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Apply mean pooling on a document level\n",
    "new_dict = {key: [np.mean(arr_list, axis=0).tolist() for arr_list in value] for key, value in tqdm_notebook(we_filtered.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "d9ac3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Apply a max pooling on the mean poolings\n",
    "we_maxpooled = {}\n",
    "for word in tqdm_notebook(new_dict) :\n",
    "    we_maxpooled[word] = np.max(new_dict[word],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "5384ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(k, v) for k, v in we_maxpooled.items()]\n",
    "df_we_maxpool = pd.DataFrame(data, columns=['content', 'embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "db10f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = calculate_cluster_embeddings(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "f7ac1eb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res_proposed = find_nearest_words(topic_vectors,df_we_maxpool,documents_per_topic_per_time,k=10,diversity=0.0,verbose=False)\n",
    "res_proposed = res_proposed.drop('embedding', axis=1)\n",
    "res_proposed = res_proposed.rename(columns={'topic_representation' : 'nearest_words'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43902ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T16:35:40.504550Z",
     "start_time": "2023-04-15T16:35:40.493030Z"
    }
   },
   "source": [
    "# Method 3 - CTFIDF Representation (used in BERTopic and ANTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "f196f1b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res_ctfidf = ctfidf_rp2(dictionary, documents_per_topic_per_time, num_doc=len(df), num_words=10)\n",
    "res_ctfidf = res_ctfidf.drop(['content','cluster','slice_num','num_doc'], axis=1)\n",
    "res_ctfidf = res_ctfidf.rename(columns={'topic_representation' : 'nearest_words'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850683a",
   "metadata": {},
   "source": [
    "# 4 - Comparing topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "aebbd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = CoherenceModel(topics=res_top2vec.nearest_words.to_list(), texts=tokens, dictionary=dictionary, coherence=\"c_v\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "9ce0824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = CoherenceModel(topics=res_proposed.nearest_words.to_list(), texts=tokens, dictionary=dictionary, coherence=\"c_v\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "0e4618c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm3 = CoherenceModel(topics=res_ctfidf.nearest_words.to_list(), texts=tokens, dictionary=dictionary, coherence=\"c_v\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "ff7e3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZERS_PARALLELISM=(True | False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "ff255f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Example lists of floats\n",
    "list1 = cm1.get_coherence_per_topic()\n",
    "list2 = cm2.get_coherence_per_topic()\n",
    "list3 = cm3.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0dc786fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T14:09:17.036170Z",
     "start_time": "2023-06-22T14:09:16.807251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "################ SCATTER PLOT ###################\n",
    "\n",
    "# Create x-axis values from 1 to the length of the lists\n",
    "x = list(range(1, len(list1) + 1))\n",
    "\n",
    "# Create a trace for the first list\n",
    "trace1 = go.Scatter(\n",
    "    x=x,\n",
    "    y=list1,\n",
    "    mode='markers',\n",
    "    name='Nearest Words (non-contextual)',\n",
    "    marker=dict(color='blue')\n",
    ")\n",
    "\n",
    "# Create a trace for the second list\n",
    "trace2 = go.Scatter(\n",
    "    x=x,\n",
    "    y=list2,\n",
    "    mode='markers',\n",
    "    name='Nearest Words (contextual)',\n",
    "    marker=dict(color='red')\n",
    ")\n",
    "\n",
    "# Create a trace for the second list\n",
    "trace3 = go.Scatter(\n",
    "    x=x,\n",
    "    y=list3,\n",
    "    mode='markers',\n",
    "    name='c-TF-IDF',\n",
    "    marker=dict(color='green')\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title='Topic coherence',\n",
    "    xaxis=dict(title='Topics'),\n",
    "    yaxis=dict(title='Coherence')\n",
    ")\n",
    "\n",
    "# Create the figure and add the traces\n",
    "fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "b5341c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ BOX PLOT ###################\n",
    "\n",
    "# Create a list to hold the box plot traces\n",
    "data = []\n",
    "\n",
    "# Create a box plot trace for list1\n",
    "trace1 = go.Box(y=list1, name='Nearest Words (non-contextual)')\n",
    "data.append(trace1)\n",
    "\n",
    "# Create a box plot trace for list2\n",
    "trace2 = go.Box(y=list2, name='Nearest Words (contextual)')\n",
    "data.append(trace2)\n",
    "\n",
    "# Create a box plot trace for list3\n",
    "trace3 = go.Box(y=list3, name='c-TF-IDF')\n",
    "data.append(trace3)\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    yaxis=dict(title='Coherence value')\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b6065",
   "metadata": {},
   "source": [
    "# 5 - Comparing topic representations using Venn Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "3044447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def showcase_topic_difference_plus(df1,df2, i) :\n",
    "    \n",
    "    list1 = df1.iloc[i]['nearest_words']\n",
    "    list2 = df2.iloc[i]['nearest_words']\n",
    "    \n",
    "    # Convert your lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Create the Venn diagram\n",
    "    venn = venn2([set1, set2], (' ', ' '))\n",
    "\n",
    "    # Determine the intersection and unique elements\n",
    "    intersection = set1 & set2\n",
    "    unique1 = set1 - intersection\n",
    "    unique2 = set2 - intersection\n",
    "\n",
    "    # Label the subsets with the actual elements instead of the counts\n",
    "    for subset in ('10', '01', '11'):\n",
    "        if venn.get_label_by_id(subset):\n",
    "            labels = '\\n'.join(sorted(intersection if subset == '11'\n",
    "                                      else unique1 if subset == '10'\n",
    "                                      else unique2))\n",
    "            venn.get_label_by_id(subset).set_text(labels)\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
